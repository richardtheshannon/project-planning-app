# Complete Implementation Guide: Application Architecture & Database Migration Strategy

## üìã Pre-Implementation Requirements

### Required Package Installation
Run this command in your VS Code terminal (CMD):
```bash
npm install --save-dev husky @types/node
```

**Why we need these packages:**
- **husky**: Provides Git hooks to validate changes before commits (prevents broken code from being pushed)
- **@types/node**: TypeScript definitions for Node.js (needed for our validation scripts)

### Current Status Checklist
- ‚úÖ Database backup exists
- ‚úÖ GitHub repository is up to date
- ‚úÖ Working Railway deployment
- ‚è≥ Required packages need to be installed

---

## üó∫Ô∏è Implementation Roadmap Overview

### **Phase 1: Foundation & Safety (Day 1-2)**
- Git configuration and backup systems
- Environment files setup
- Package.json scripts (non-breaking additions)
- Documentation structure

### **Phase 2: Database Safety Net (Day 3-4)**
- Migration validation scripts
- Backup/restore scripts
- Rollback capabilities
- Shadow database setup

### **Phase 3: Project Structure (Week 2)**
- Create new folder structure (parallel to existing)
- Service layer implementation
- Move components gradually

### **Phase 4: Feature Modularization (Week 3-4)**
- Refactor features one by one
- Update imports progressively
- Test each change

---

## PHASE 1: Foundation & Safety Setup

### Step 1.1: Install Required Packages

```bash
# Run in VS Code terminal (CMD)
npm install --save-dev husky @types/node

# Initialize husky (creates .husky folder)
npx husky install
```

### Step 1.2: Create Project Directories

Run these commands in VS Code terminal:
```cmd
mkdir backups
mkdir scripts
mkdir logs
mkdir documentation
echo. > backups\.gitkeep
echo. > logs\.gitkeep
```

### Step 1.3: Update .gitignore

**ACTION REQUIRED:** Before updating, save your current .gitignore content.

Add these lines to your `.gitignore` file:
```gitignore
# === NEW SAFETY ADDITIONS ===

# Build outputs (if not already present)
.next/
out/

# Cache files (important to add)
.next/cache/
*.pack.gz
*.pack

# Prisma generated files
prisma/src/

# Environment files (keep all environments separate)
.env.local
.env.production
.env.development
.env.staging

# Backup files (don't commit database backups)
backups/*.sql
!backups/.gitkeep

# Log files
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# OS Files
.DS_Store
Thumbs.db
desktop.ini

# IDE
.vscode/settings.json
.idea/

# Temporary files
*.tmp
*.temp
```

### Step 1.4: Create Environment Files

Create `.env.example` (this is safe to commit):
```env
# === DATABASE ===
DATABASE_URL="mysql://user:password@localhost:3306/businessapp_dev"
SHADOW_DATABASE_URL="mysql://user:password@localhost:3306/businessapp_shadow"

# === AUTHENTICATION ===
NEXTAUTH_SECRET="generate-a-secret-here"
NEXTAUTH_URL="http://localhost:3000"

# === GOOGLE OAUTH (if using) ===
GOOGLE_CLIENT_ID=""
GOOGLE_CLIENT_SECRET=""

# === EMAIL CONFIGURATION (if using) ===
EMAIL_FROM="noreply@example.com"
SMTP_HOST=""
SMTP_PORT="587"
SMTP_USER=""
SMTP_PASS=""

# === FEATURE FLAGS ===
ENABLE_EMAIL="false"
ENABLE_UPLOADS="true"
ENABLE_MANIFEST="false"
ENABLE_BACKUPS="true"

# === ENVIRONMENT ===
NODE_ENV="development"
MIGRATION_MODE="development"
```

Create `.env.development` (copy your current .env and add):
```env
# Your existing .env content here
# Plus add:
MIGRATION_MODE="development"
ENABLE_BACKUPS="true"
```

### Step 1.5: Update package.json Scripts

**ACTION REQUIRED:** Add these scripts to your package.json "scripts" section:

```json
{
  "scripts": {
    // ... keep all your existing scripts ...
    
    // === NEW SAFE SCRIPTS (add these) ===
    
    // Database status and validation
    "db:status": "prisma migrate status",
    "db:validate": "node ./scripts/validate-migration.js",
    
    // Backup scripts
    "db:backup": "node ./scripts/backup-database.js",
    "db:backup:list": "dir backups\\*.sql",
    
    // Development helpers
    "dev:clean": "rmdir /s /q .next 2>nul || echo Cache cleaned",
    "dev:fresh": "npm run dev:clean && npm run dev",
    
    // Pre-deployment checks
    "predeploy:check": "npm run db:validate && npm run db:backup",
    
    // Documentation
    "docs:structure": "node ./scripts/analyze-structure.js"
  }
}
```

---

## PHASE 2: Database Safety Implementation

### Step 2.1: Create Migration Validation Script

Create file: `scripts/validate-migration.js`

```javascript
/**
 * Migration Validation Script
 * Checks for dangerous operations in pending migrations
 */

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

async function validateMigration() {
  console.log('üîç Validating pending migrations...');
  console.log('‚ïê'.repeat(50));
  
  try {
    // Check for pending migrations
    let status;
    try {
      status = execSync('npx prisma migrate status', { 
        encoding: 'utf8',
        shell: 'cmd.exe',
        stdio: ['pipe', 'pipe', 'pipe']
      });
    } catch (error) {
      // Status command returns non-zero when migrations are pending
      status = error.stdout || error.stderr || '';
    }
    
    if (status.includes('Database schema is up to date')) {
      console.log('‚úÖ No pending migrations');
      return { success: true, warnings: [], errors: [] };
    }
    
    // Get migrations directory
    const migrationsDir = path.join(__dirname, '..', 'prisma', 'migrations');
    
    if (!fs.existsSync(migrationsDir)) {
      console.log('üìÅ No migrations directory found');
      return { success: true, warnings: [], errors: [] };
    }
    
    // Find migration folders
    const migrations = fs.readdirSync(migrationsDir)
      .filter(f => f.match(/^\d{14}_/))
      .sort();
    
    if (migrations.length === 0) {
      console.log('üìÑ No migrations found');
      return { success: true, warnings: [], errors: [] };
    }
    
    // Analyze latest migration
    const latestMigration = migrations[migrations.length - 1];
    const migrationSQLPath = path.join(migrationsDir, latestMigration, 'migration.sql');
    
    if (!fs.existsSync(migrationSQLPath)) {
      console.log('‚ö†Ô∏è  Migration SQL file not found for:', latestMigration);
      return { success: true, warnings: ['SQL file not found'], errors: [] };
    }
    
    const migrationSQL = fs.readFileSync(migrationSQLPath, 'utf8');
    
    console.log(`\nüìÑ Analyzing migration: ${latestMigration}`);
    console.log('‚îÄ'.repeat(50));
    
    // Validation rules
    const validationRules = [
      {
        pattern: /DROP\s+TABLE/i,
        severity: 'error',
        message: 'DROP TABLE detected - this will permanently delete table and all data!'
      },
      {
        pattern: /DROP\s+DATABASE/i,
        severity: 'error',
        message: 'DROP DATABASE detected - this will destroy entire database!'
      },
      {
        pattern: /TRUNCATE/i,
        severity: 'error',
        message: 'TRUNCATE detected - this will delete all data in table!'
      },
      {
        pattern: /DROP\s+COLUMN/i,
        severity: 'warning',
        message: 'DROP COLUMN detected - ensure data is backed up'
      },
      {
        pattern: /ALTER\s+TABLE\s+\w+\s+MODIFY/i,
        severity: 'warning',
        message: 'Column modification detected - verify data compatibility'
      },
      {
        pattern: /DELETE\s+FROM/i,
        severity: 'warning',
        message: 'DELETE operation detected - verify this is intentional'
      },
      {
        pattern: /ALTER\s+TABLE\s+\w+\s+RENAME/i,
        severity: 'warning',
        message: 'Table rename detected - update all references'
      }
    ];
    
    const errors = [];
    const warnings = [];
    
    // Check each rule
    validationRules.forEach(rule => {
      if (rule.pattern.test(migrationSQL)) {
        const icon = rule.severity === 'error' ? '‚ùå' : '‚ö†Ô∏è';
        console.log(`${icon} ${rule.message}`);
        
        if (rule.severity === 'error') {
          errors.push(rule.message);
        } else {
          warnings.push(rule.message);
        }
      }
    });
    
    // Summary
    console.log('\n' + '‚ïê'.repeat(50));
    if (errors.length > 0) {
      console.error(`\n‚ùå VALIDATION FAILED: ${errors.length} error(s) found`);
      console.log('Fix these issues before proceeding.');
      process.exit(1);
    } else if (warnings.length > 0) {
      console.log(`\n‚ö†Ô∏è  VALIDATION PASSED WITH WARNINGS: ${warnings.length} warning(s)`);
      console.log('Review warnings and ensure you have backups.');
    } else {
      console.log('\n‚úÖ VALIDATION PASSED: Migration looks safe!');
    }
    
    return { success: errors.length === 0, warnings, errors };
    
  } catch (error) {
    console.error('‚ùå Validation script error:', error.message);
    process.exit(1);
  }
}

// Run if called directly
if (require.main === module) {
  validateMigration();
}

module.exports = { validateMigration };
```

### Step 2.2: Create Database Backup Script

Create file: `scripts/backup-database.js`

```javascript
/**
 * Database Backup Script
 * Creates timestamped backups of your MySQL database
 */

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

function parseConnectionString(url) {
  // Handle various MySQL URL formats
  const patterns = [
    // Standard MySQL URL
    /mysql:\/\/([^:]+):([^@]+)@([^:]+):(\d+)\/(.+)/,
    // Railway MySQL URL (may have additional parameters)
    /mysql:\/\/([^:]+):([^@]+)@([^:]+):(\d+)\/([^?]+)/
  ];
  
  for (const pattern of patterns) {
    const match = url.match(pattern);
    if (match) {
      return {
        user: match[1],
        password: match[2],
        host: match[3],
        port: match[4],
        database: match[5]
      };
    }
  }
  
  throw new Error('Could not parse DATABASE_URL');
}

async function checkMySQLTools() {
  try {
    execSync('where mysql', { shell: 'cmd.exe', stdio: 'ignore' });
    execSync('where mysqldump', { shell: 'cmd.exe', stdio: 'ignore' });
    return true;
  } catch {
    console.error('‚ùå MySQL command-line tools not found!');
    console.log('\nüì• Please install MySQL tools:');
    console.log('1. Visit: https://dev.mysql.com/downloads/installer/');
    console.log('2. Download "MySQL Installer for Windows"');
    console.log('3. During installation, select "MySQL Shell" and "MySQL Workbench"');
    console.log('4. Add MySQL bin directory to your PATH');
    console.log('   (Usually: C:\\Program Files\\MySQL\\MySQL Server 8.0\\bin)');
    console.log('\nüí° Alternative: Use Railway\'s database backup feature');
    return false;
  }
}

async function backupDatabase(options = {}) {
  console.log('üîÑ Starting database backup...');
  console.log('‚ïê'.repeat(50));
  
  // Check for MySQL tools
  if (!await checkMySQLTools()) {
    return null;
  }
  
  const timestamp = new Date().toISOString()
    .replace(/[:.]/g, '-')
    .replace('T', '_')
    .slice(0, -5);
  
  const backupDir = path.join(__dirname, '..', 'backups');
  
  // Create backup directory
  if (!fs.existsSync(backupDir)) {
    fs.mkdirSync(backupDir, { recursive: true });
    console.log('üìÅ Created backup directory');
  }
  
  const backupFile = path.join(backupDir, `backup_${timestamp}.sql`);
  
  // Get database URL
  const dbUrl = process.env.DATABASE_URL;
  if (!dbUrl) {
    console.error('‚ùå DATABASE_URL not found in environment');
    console.log('üí° Make sure your .env file is loaded');
    return null;
  }
  
  try {
    const db = parseConnectionString(dbUrl);
    
    console.log(`\nüìä Database Details:`);
    console.log(`   Database: ${db.database}`);
    console.log(`   Host: ${db.host}`);
    console.log(`   Port: ${db.port}`);
    console.log(`   User: ${db.user}`);
    
    // Build mysqldump command
    const mysqldumpArgs = [
      `-h${db.host}`,
      `-P${db.port}`,
      `-u${db.user}`,
      `-p${db.password}`,
      '--single-transaction',  // For InnoDB tables
      '--routines',            // Include stored procedures
      '--triggers',            // Include triggers
      '--add-drop-table',      // Add DROP TABLE before CREATE
      '--create-options',      // Include all table options
      '--extended-insert',     // Use multiple-row INSERT
      '--lock-tables=false',   // Don't lock tables
      '--quick',              // Don't buffer query results
      db.database
    ];
    
    console.log('\n‚è≥ Creating backup (this may take a moment)...');
    
    // Execute mysqldump
    const command = `mysqldump ${mysqldumpArgs.join(' ')} > "${backupFile}"`;
    
    execSync(command, { 
      shell: 'cmd.exe',
      stdio: 'inherit',
      windowsHide: true
    });
    
    // Verify backup was created
    if (!fs.existsSync(backupFile)) {
      throw new Error('Backup file was not created');
    }
    
    // Get file size
    const stats = fs.statSync(backupFile);
    const fileSizeInMB = (stats.size / (1024 * 1024)).toFixed(2);
    
    // Success message
    console.log('\n' + '‚ïê'.repeat(50));
    console.log('‚úÖ Backup completed successfully!');
    console.log(`üìÅ File: ${path.basename(backupFile)}`);
    console.log(`üìä Size: ${fileSizeInMB} MB`);
    console.log(`üìç Location: ${backupFile}`);
    
    // Cleanup old backups (keep last 10)
    const backups = fs.readdirSync(backupDir)
      .filter(f => f.startsWith('backup_') && f.endsWith('.sql'))
      .sort();
    
    if (backups.length > 10) {
      console.log('\nüßπ Cleaning up old backups...');
      const toDelete = backups.slice(0, backups.length - 10);
      toDelete.forEach(file => {
        fs.unlinkSync(path.join(backupDir, file));
        console.log(`   Deleted: ${file}`);
      });
    }
    
    return backupFile;
    
  } catch (error) {
    console.error('\n‚ùå Backup failed:', error.message);
    
    if (error.message.includes('Access denied')) {
      console.log('üìå Check your database credentials');
    } else if (error.message.includes('Unknown database')) {
      console.log('üìå Check your database name');
    } else if (error.message.includes('connect')) {
      console.log('üìå Check your database host and port');
      console.log('üìå Make sure the database is accessible');
    }
    
    return null;
  }
}

// Run if called directly
if (require.main === module) {
  require('dotenv').config();
  backupDatabase();
}

module.exports = { backupDatabase, parseConnectionString };
```

### Step 2.3: Create Structure Analysis Script

Create file: `scripts/analyze-structure.js`

```javascript
/**
 * Project Structure Analysis Script
 * Analyzes current project structure and identifies issues
 */

const fs = require('fs');
const path = require('path');

function analyzeStructure() {
  console.log('üìä Analyzing Project Structure...');
  console.log('‚ïê'.repeat(50));
  
  const projectRoot = path.join(__dirname, '..');
  const issues = [];
  const suggestions = [];
  
  // Check for large files that should be modularized
  function checkFileSize(filePath, maxSizeKB = 100) {
    if (fs.existsSync(filePath)) {
      const stats = fs.statSync(filePath);
      const sizeKB = stats.size / 1024;
      if (sizeKB > maxSizeKB) {
        return { 
          size: sizeKB, 
          oversized: true 
        };
      }
    }
    return { size: 0, oversized: false };
  }
  
  // Analyze src/app structure
  console.log('\nüìÅ Checking App Directory...');
  const appDir = path.join(projectRoot, 'src', 'app');
  const dashboardPage = path.join(appDir, 'dashboard', 'page.tsx');
  
  if (fs.existsSync(dashboardPage)) {
    const sizeCheck = checkFileSize(dashboardPage);
    if (sizeCheck.oversized) {
      issues.push(`Dashboard page is ${sizeCheck.size.toFixed(0)}KB - should be < 100KB`);
      suggestions.push('Break dashboard/page.tsx into smaller components');
    }
  }
  
  // Check for mixed concerns in API routes
  console.log('\nüîå Checking API Routes...');
  const apiDir = path.join(appDir, 'api');
  if (fs.existsSync(apiDir)) {
    const apiRoutes = fs.readdirSync(apiDir);
    const largeRoutes = [];
    
    apiRoutes.forEach(route => {
      const routePath = path.join(apiDir, route, 'route.ts');
      const sizeCheck = checkFileSize(routePath, 50);
      if (sizeCheck.oversized) {
        largeRoutes.push(route);
      }
    });
    
    if (largeRoutes.length > 0) {
      issues.push(`Large API routes found: ${largeRoutes.join(', ')}`);
      suggestions.push('Extract business logic from API routes into service layer');
    }
  }
  
  // Check for missing directories
  console.log('\nüìÇ Checking Project Organization...');
  const recommendedDirs = [
    'src/services',
    'src/features',
    'src/types',
    'src/config',
    'documentation',
    'scripts',
    'backups'
  ];
  
  const missingDirs = [];
  recommendedDirs.forEach(dir => {
    const dirPath = path.join(projectRoot, dir);
    if (!fs.existsSync(dirPath)) {
      missingDirs.push(dir);
    }
  });
  
  if (missingDirs.length > 0) {
    suggestions.push(`Create missing directories: ${missingDirs.join(', ')}`);
  }
  
  // Check for build artifacts that should be gitignored
  console.log('\nüóëÔ∏è Checking for Build Artifacts...');
  const artifactsToIgnore = [
    '.next/cache/webpack',
    'prisma/src',
    '.next/cache/images'
  ];
  
  const foundArtifacts = [];
  artifactsToIgnore.forEach(artifact => {
    const artifactPath = path.join(projectRoot, artifact);
    if (fs.existsSync(artifactPath)) {
      foundArtifacts.push(artifact);
    }
  });
  
  if (foundArtifacts.length > 0) {
    issues.push('Build artifacts found that should be gitignored');
    suggestions.push(`Add to .gitignore: ${foundArtifacts.join(', ')}`);
  }
  
  // Check component organization
  console.log('\nüß© Checking Component Organization...');
  const componentsDir = path.join(projectRoot, 'src', 'components');
  if (fs.existsSync(componentsDir)) {
    const componentFolders = fs.readdirSync(componentsDir)
      .filter(f => fs.statSync(path.join(componentsDir, f)).isDirectory());
    
    if (componentFolders.length > 20) {
      issues.push(`Too many component folders (${componentFolders.length}) - needs better organization`);
      suggestions.push('Group related components into feature folders');
    }
  }
  
  // Report findings
  console.log('\n' + '‚ïê'.repeat(50));
  console.log('üìã ANALYSIS REPORT\n');
  
  if (issues.length > 0) {
    console.log('‚ùå Issues Found:');
    issues.forEach((issue, i) => {
      console.log(`   ${i + 1}. ${issue}`);
    });
  } else {
    console.log('‚úÖ No major issues found!');
  }
  
  if (suggestions.length > 0) {
    console.log('\nüí° Suggestions:');
    suggestions.forEach((suggestion, i) => {
      console.log(`   ${i + 1}. ${suggestion}`);
    });
  }
  
  // Create report file
  const report = {
    timestamp: new Date().toISOString(),
    issues,
    suggestions,
    stats: {
      totalIssues: issues.length,
      totalSuggestions: suggestions.length
    }
  };
  
  const reportPath = path.join(projectRoot, 'documentation', 'structure-analysis.json');
  if (!fs.existsSync(path.dirname(reportPath))) {
    fs.mkdirSync(path.dirname(reportPath), { recursive: true });
  }
  
  fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));
  console.log(`\nüìÑ Full report saved to: documentation/structure-analysis.json`);
  
  return report;
}

// Run if called directly
if (require.main === module) {
  analyzeStructure();
}

module.exports = { analyzeStructure };
```

---

## PHASE 3: Project Structure Setup (Do NOT Delete Existing Files!)

### Step 3.1: Create New Feature-Based Structure

**IMPORTANT:** We're creating a NEW structure alongside your existing code. Do NOT delete anything!

Create these directories:
```cmd
mkdir src\features
mkdir src\features\auth
mkdir src\features\dashboard
mkdir src\features\documents
mkdir src\features\financials
mkdir src\features\projects
mkdir src\features\settings
mkdir src\features\team

mkdir src\services
mkdir src\services\auth
mkdir src\services\email
mkdir src\services\financial
mkdir src\services\storage

mkdir src\config
mkdir src\lib\errors
mkdir src\lib\api
mkdir src\lib\performance
mkdir src\lib\security
```

### Step 3.2: Create Base Service Layer

Create file: `src/services/base.service.ts`

```typescript
/**
 * Base Service Class
 * Provides common functionality for all services
 */

import { PrismaClient } from '@prisma/client';
import { prisma } from '@/lib/prisma';

export abstract class BaseService {
  protected prisma: PrismaClient;
  
  constructor() {
    this.prisma = prisma;
  }
  
  /**
   * Log service actions for debugging
   */
  protected log(action: string, data?: any) {
    if (process.env.NODE_ENV === 'development') {
      console.log(`[${this.constructor.name}] ${action}`, data || '');
    }
  }
  
  /**
   * Handle service errors consistently
   */
  protected handleError(error: any, context: string) {
    console.error(`[${this.constructor.name}] Error in ${context}:`, error);
    throw error;
  }
}
```

### Step 3.3: Create Configuration Module

Create file: `src/config/app.config.ts`

```typescript
/**
 * Application Configuration
 * Centralizes all configuration in one place
 */

export const appConfig = {
  // Application
  app: {
    name: process.env.NEXT_PUBLIC_APP_NAME || 'Business Manager',
    url: process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000',
    env: process.env.NODE_ENV || 'development',
  },
  
  // Database
  database: {
    url: process.env.DATABASE_URL!,
    shadowUrl: process.env.SHADOW_DATABASE_URL,
  },
  
  // Authentication
  auth: {
    secret: process.env.NEXTAUTH_SECRET!,
    url: process.env.NEXTAUTH_URL || 'http://localhost:3000',
    providers: {
      google: {
        clientId: process.env.GOOGLE_CLIENT_ID || '',
        clientSecret: process.env.GOOGLE_CLIENT_SECRET || '',
      },
    },
  },
  
  // Email
  email: {
    enabled: process.env.ENABLE_EMAIL === 'true',
    from: process.env.EMAIL_FROM || 'noreply@example.com',
    smtp: {
      host: process.env.SMTP_HOST || '',
      port: parseInt(process.env.SMTP_PORT || '587'),
      user: process.env.SMTP_USER || '',
      pass: process.env.SMTP_PASS || '',
    },
  },
  
  // Features
  features: {
    emailNotifications: process.env.ENABLE_EMAIL === 'true',
    fileUploads: process.env.ENABLE_UPLOADS === 'true',
    dailyManifest: process.env.ENABLE_MANIFEST === 'true',
    backups: process.env.ENABLE_BACKUPS === 'true',
  },
  
  // Storage
  storage: {
    uploadDir: process.env.UPLOAD_DIR || 'public/uploads',
    maxFileSize: parseInt(process.env.MAX_FILE_SIZE || '10485760'), // 10MB default
  },
};
```

---

## PHASE 4: Safe Migration Workflow

### Step 4.1: Create Git Hooks

Create file: `.husky/pre-commit`

```bash
#!/bin/sh
. "$(dirname "$0")/_/husky.sh"

echo "üîç Running pre-commit checks..."

# Check for database changes
if git diff --cached --name-only | grep -q "prisma/schema.prisma"; then
  echo "üìä Schema changes detected - validating..."
  npm run db:validate
  
  if [ $? -ne 0 ]; then
    echo "‚ùå Validation failed! Fix issues before committing."
    exit 1
  fi
fi

echo "‚úÖ Pre-commit checks passed!"
```

### Step 4.2: Create Railway Configuration

Create file: `railway.json`

```json
{
  "build": {
    "builder": "NIXPACKS",
    "buildCommand": "npm ci && npx prisma generate"
  },
  "deploy": {
    "startCommand": "npm run db:migrate:apply && npm start",
    "healthcheckPath": "/api/health",
    "healthcheckTimeout": 300,
    "restartPolicyType": "ON_FAILURE",
    "restartPolicyMaxRetries": 3
  }
}
```

### Step 4.3: Create Health Check Endpoint

Create file: `src/app/api/health/route.ts`

```typescript
import { NextResponse } from 'next/server';
import { prisma } from '@/lib/prisma';

export async function GET() {
  try {
    // Check database connection
    await prisma.$queryRaw`SELECT 1`;
    
    return NextResponse.json({
      status: 'healthy',
      timestamp: new Date().toISOString(),
      database: 'connected',
    });
  } catch (error) {
    return NextResponse.json(
      {
        status: 'unhealthy',
        timestamp: new Date().toISOString(),
        database: 'disconnected',
        error: error instanceof Error ? error.message : 'Unknown error',
      },
      { status: 503 }
    );
  }
}
```

---

## Testing & Verification Steps

### After Each Phase, Run These Tests:

#### Phase 1 Tests:
```bash
# Test new scripts exist
npm run db:status
npm run db:backup:list

# Verify directories created
dir backups
dir scripts
dir logs
```

#### Phase 2 Tests:
```bash
# Test validation script
npm run db:validate

# Test backup (if MySQL tools installed)
npm run db:backup

# Test structure analysis
npm run docs:structure
```

#### Phase 3 Tests:
```bash
# Verify new directories
dir src\features
dir src\services
dir src\config

# Check TypeScript compilation
npx tsc --noEmit
```

#### Phase 4 Tests:
```bash
# Test health endpoint
curl http://localhost:3000/api/health

# Test git hooks
git add .
git commit -m "test: pre-commit hook"
```

---

## Rollback Plan

If anything goes wrong at any phase:

### Phase 1 Rollback:
```bash
# Remove new directories (if needed)
rmdir /s /q backups
rmdir /s /q scripts
rmdir /s /q logs

# Restore package.json from git
git checkout -- package.json
```

### Phase 2 Rollback:
```bash
# Remove scripts
del scripts\*.js

# Restore package.json
git checkout -- package.json
```

### Phase 3 Rollback:
```bash
# Remove new directories only
rmdir /s /q src\features
rmdir /s /q src\services
rmdir /s /q src\config
```

### Phase 4 Rollback:
```bash
# Remove hooks
rmdir /s /q .husky

# Remove new config files
del railway.json
del src\app\api\health\route.ts
```

---

## Daily Development Workflow

After implementation, your daily workflow will be:

### Before Starting Work:
```bash
# 1. Pull latest changes
git pull

# 2. Check migration status
npm run db:status

# 3. Backup database (optional but recommended)
npm run db:backup
```

### When Making Database Changes:
```bash
# 1. Create migration
npx prisma migrate dev --create-only --name your_change_name

# 2. Validate migration
npm run db:validate

# 3. Apply migration locally
npx prisma migrate dev

# 4. Test thoroughly
npm test
```

### Before Deploying:
```bash
# 1. Run pre-deployment check
npm run predeploy:check

# 2. Commit changes
git add .
git commit -m "feat: your feature"

# 3. Push to GitHub
git push

# Railway handles the rest!
```

---

## Support & Troubleshooting

### Common Issues:

**Issue: MySQL tools not found**
- Solution: Install MySQL Workbench or use Railway's backup feature

**Issue: Migration validation fails**
- Solution: Review the migration SQL and ensure no destructive operations

**Issue: Railway deployment fails**
- Solution: Check Railway logs and ensure migrations were tested locally

**Issue: Git hooks not running**
- Solution: Run `npx husky install` to reinitialize

---

## Success Checklist

After full implementation, you should have:

- [ ] ‚úÖ Automated backup system
- [ ] ‚úÖ Migration validation before commits
- [ ] ‚úÖ Organized project structure
- [ ] ‚úÖ Service layer for business logic
- [ ] ‚úÖ Health monitoring endpoint
- [ ] ‚úÖ Safe deployment workflow
- [ ] ‚úÖ Rollback capabilities
- [ ] ‚úÖ Documentation system

---

## Next Steps After Implementation

1. **Start refactoring gradually**: Move one component at a time to the new structure
2. **Document as you go**: Add README files to each feature folder
3. **Add tests**: Create test files alongside new services
4. **Monitor performance**: Use the health endpoint to track app health
5. **Regular backups**: Run backups before major changes

---

## Questions to Answer Before Starting

Please confirm before we begin implementation:

1. **Do you want to proceed with Phase 1 first?** (Safest option)
2. **Do you have MySQL command-line tools installed?** (For backups)
3. **What's your Railway project name?** (For configuration)
4. **Do you want to set up staging environment?** (Recommended)

---

## Contact & Support

If you encounter any issues during implementation:

1. **Check the error messages carefully**
2. **Review the rollback procedures**
3. **Make sure all files are saved**
4. **Verify your Railway deployment is stable**
5. **Keep backups of everything**

Remember: We're adding to your project, not replacing it. Everything is reversible!

---

**Ready to start? Begin with Phase 1 - it's completely safe and won't affect your existing code!**